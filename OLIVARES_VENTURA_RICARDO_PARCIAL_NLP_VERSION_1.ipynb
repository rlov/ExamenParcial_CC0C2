{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RsctffnJGf6"
      },
      "source": [
        "# OLIVARES VENTURA RICARDO LEONARDO\n",
        "# CÓDIGO: 20192002A\n",
        "# EXAMEN PARCIAL DE PROCESAMIENTO DEL LENGUAJE NATURAL - VERSIÓN 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZjIPIiEKvop"
      },
      "source": [
        "# 0.1.1 Ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGmpf6fuLAaz"
      },
      "source": [
        "## Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt5Zla5bKz5z"
      },
      "source": [
        "Dadas tres oraciones \"all models are wrong\", a model is wrong y some models are useful, y\n",
        "un vocabulario {< s >, < /s >, a, all, are, model, models, some, useful, wrong}. En codigo responde las siguientes preguntas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QauYHUpBJALt",
        "outputId": "3f4f854f-b8e3-40dd-8d0f-381f9573a0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a) Probabilidad de todos los bigramas sin suavizado:  {('all', 'models'): 1.0, ('models', 'are'): 1.0, ('are', 'wrong'): 0.5, ('a', 'model'): 1.0, ('model', 'is'): 1.0, ('is', 'wrong'): 1.0, ('some', 'models'): 1.0, ('are', 'useful'): 0.5}\n",
            "b.1) Probabilidad de todos los bigramas son suavizado de laplace o add-one:  {('all', 'models'): 0.2, ('models', 'are'): 0.2727272727272727, ('are', 'wrong'): 0.18181818181818182, ('a', 'model'): 0.2, ('model', 'is'): 0.2, ('is', 'wrong'): 0.2, ('some', 'models'): 0.2, ('are', 'useful'): 0.18181818181818182}\n",
            "b.2) Probabilidad del bigrama no visto 'a models' es:  0.1\n",
            "c.1) Probabilidad de todos los bigramas son suavizado add-k = 0.05 son:  {('all', 'models'): 0.7241379310344828, ('models', 'are'): 0.8367346938775508, ('are', 'wrong'): 0.42857142857142855, ('a', 'model'): 0.7241379310344828, ('model', 'is'): 0.7241379310344828, ('is', 'wrong'): 0.7241379310344828, ('some', 'models'): 0.7241379310344828, ('are', 'useful'): 0.42857142857142855}\n",
            "c.1) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.05 es:  0.034482758620689655\n",
            "c.2) Probabilidad de todos los bigramas son suavizado add-k = 0.15 son:  {('all', 'models'): 0.4893617021276596, ('models', 'are'): 0.6417910447761195, ('are', 'wrong'): 0.34328358208955223, ('a', 'model'): 0.4893617021276596, ('model', 'is'): 0.4893617021276596, ('is', 'wrong'): 0.4893617021276596, ('some', 'models'): 0.4893617021276596, ('are', 'useful'): 0.34328358208955223}\n",
            "c.2) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.15 es:  0.06382978723404256\n",
            "d.1) Probabilidad de todos los bigramas backoff:  {('all', 'models'): 1.0, ('models', 'are'): 1.0, ('are', 'wrong'): 0.5, ('a', 'model'): 1.0, ('model', 'is'): 1.0, ('is', 'wrong'): 1.0, ('some', 'models'): 1.0, ('are', 'useful'): 0.5}\n",
            "d.1) Probabilidad del bigrama no visto 'a models' es:  0.2222222222222222\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "vocabulary = {\n",
        "    \"<s/>\",\n",
        "    \"a\",\n",
        "    \"all\",\n",
        "    \"are\",\n",
        "    \"model\",\n",
        "    \"models\",\n",
        "    \"some\",\n",
        "    \"useful\",\n",
        "    \"wrong\"\n",
        "}\n",
        "\n",
        "# a) Calcula las probabilidades de todos los bigramas sin suavizado\n",
        "# Preprocesamos el texto y convertimos todo a minúscula\n",
        "# Para poder estandarizar y que no detecte como dos palabras\n",
        "# distintas por ser mayúscula y munúscula\n",
        "def preprocess_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def calculate_bigram_probabilities(corpus):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama, la cual es\n",
        "        # la cantidad de veces que aparece un bigrama entre el unigrama que precede al último elemento\n",
        "        # del bigrama\n",
        "        bigram_probabilities[bigram] = count / unigram_counts[unigram]\n",
        "\n",
        "    return bigram_probabilities\n",
        "\n",
        "bigram_probabilities = calculate_bigram_probabilities(corpus)\n",
        "\n",
        "print(\"a) Probabilidad de todos los bigramas sin suavizado: \", bigram_probabilities)\n",
        "\n",
        "# b) Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con suavizado add-one.\n",
        "bigrama_doesnt_exist = (\"a\", \"models\")\n",
        "\n",
        "# Aquí aplicaremos el suavizado add-one o también llamado suavizado de Laplace,\n",
        "# el cual es útil cuando hay bigramas que no existen en el corpus de entrenamiento\n",
        "# y por ende se le asigna una probabilidad de cero, pero justamente este suavizado\n",
        "# lo que hace es que suma 1 al numerador y |v| (tamaño del vocabulario) al denominador\n",
        "# haciendo que los bigramas que no existan en el corpus de entrenamiento tengan una probabilidad\n",
        "# distinta de cero\n",
        "\n",
        "def generate_n_grams(corpus, n):\n",
        "    n_grams = []\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            n_gram = tuple(tokens[i:i+n])\n",
        "            n_grams.append(n_gram)\n",
        "    return n_grams\n",
        "\n",
        "def calculate_bigram_probabilities_with_add_one(corpus, vocabulary, word1, word2):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "        # suavizado de Laplace\n",
        "        bigram_probabilities[bigram] = (count + 1) / (unigram_counts[unigram] + len(vocabulary))\n",
        "\n",
        "        if word1 != \"\" and word2 != \"\":\n",
        "          unigram_count = unigram_counts[(word1)]\n",
        "          return (0 + 1) / (unigram_count + len(vocabulary))\n",
        "    return bigram_probabilities\n",
        "\n",
        "print(\"b.1) Probabilidad de todos los bigramas son suavizado de laplace o add-one: \",calculate_bigram_probabilities_with_add_one(corpus, vocabulary, \"\", \"\"))\n",
        "print(\"b.2) Probabilidad del bigrama no visto 'a models' es: \",calculate_bigram_probabilities_with_add_one(corpus, vocabulary, \"a\", \"models\"))\n",
        "\n",
        "# c) Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con suavizado add-k. Pruebe k = 0.05 y k =0.15.\n",
        "\n",
        "def calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, k, word1, word2):\n",
        "    bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "    # que aparezca en el corpus\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "    # k = 0.15\n",
        "\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess_text(sentence)\n",
        "        for i in range(len(tokens)):\n",
        "            unigram = tokens[i]\n",
        "            total_unigrams += 1\n",
        "            unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = (unigram, tokens[i + 1])\n",
        "                bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        unigram = bigram[0]\n",
        "        # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "        # suavizado add-k\n",
        "        bigram_probabilities[bigram] = (count + k) / (unigram_counts[unigram] + k * len(vocabulary))\n",
        "        if word1 != \"\" and word2 != \"\":\n",
        "          unigram_count = unigram_counts[(word1)]\n",
        "          return (0 + k) / (unigram_count + k * len(vocabulary))\n",
        "    return bigram_probabilities\n",
        "\n",
        "print(\"c.1) Probabilidad de todos los bigramas son suavizado add-k = 0.05 son: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.05, \"\", \"\"))\n",
        "print(\"c.1) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.05 es: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.05, \"a\", \"models\"))\n",
        "print(\"c.2) Probabilidad de todos los bigramas son suavizado add-k = 0.15 son: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.15, \"\", \"\"))\n",
        "print(\"c.2) Probabilidad del bigrama no visto 'a models' son suavizado add-k = 0.15 es: \",calculate_bigram_probabilitis_with_add_k(corpus, vocabulary, 0.15, \"a\", \"models\"))\n",
        "\n",
        "# d) Calcula las probabilidades de todo los bigramas y el bigrama\n",
        "# no visto a models con back-off y stupid-backoff\n",
        "\n",
        "def calculate_bigram_probabilities_with_backoff(corpus, vocabulary, word1, word2):\n",
        "  bigram_counts = {} # Diccionario para almacenar la frecuencia de cada bigrama\n",
        "  # que aparezca en el corpus\n",
        "  unigram_counts = {}\n",
        "  total_unigrams = 0\n",
        "  # k = 0.15\n",
        "\n",
        "  for sentence in corpus:\n",
        "      tokens = preprocess_text(sentence)\n",
        "      for i in range(len(tokens)):\n",
        "          unigram = tokens[i]\n",
        "          total_unigrams += 1\n",
        "          unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "          if i < len(tokens) - 1:\n",
        "              bigram = (unigram, tokens[i + 1])\n",
        "              bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "  bigram_probabilities = {}\n",
        "  for bigram, count in bigram_counts.items():\n",
        "      unigram = bigram[0]\n",
        "      # Aquí calculamos la probabilidad del bigrama aplicando el\n",
        "      # suavizado add-k\n",
        "      if count > 0:\n",
        "        bigram_probabilities[bigram] = count/unigram_counts[unigram]\n",
        "      else:\n",
        "        bigram_probabilities[bigram] = unigram_counts[bigram[1]] / len(vocabulary)\n",
        "\n",
        "      if word1 != \"\" and word2 != \"\":\n",
        "        return unigram_counts[bigram[1]] / len(vocabulary)\n",
        "  return bigram_probabilities\n",
        "\n",
        "print(\"d.1) Probabilidad de todos los bigramas backoff: \",calculate_bigram_probabilities_with_backoff(corpus, vocabulary, \"\", \"\"))\n",
        "print(\"d.1) Probabilidad con back-off del bigrama no visto 'a models' es: \",calculate_bigram_probabilities_with_backoff(corpus, vocabulary, \"a\", \"models\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdZLmmaLFuM"
      },
      "source": [
        "## Parte 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY4S7qiaX6zw",
        "outputId": "b1461872-0447-4f17-cff6-7cfc02434980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigramas N_r y r_star: \n",
            "r: 1, N_r: 6, r_star: 1.0\n",
            "r: 2, N_r: 3, r_star: N/A\n"
          ]
        }
      ],
      "source": [
        "# e) El suavizado de Good-Turing reasigna la masa de\n",
        "# probabilidad de los n-gramas ricos a los n-gramas pobres.\n",
        "# Dado un corpus D, supongamos que tratamos todas las unigrama\n",
        "# desconocidos como (UNK), por lo tanto, el vocabulario es\n",
        "# {w: w e D} U {<UNK>} y No = 1. Calcula r, Nr para todas las unigramas de la parte 1\n",
        "\n",
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "vocabulary = {\n",
        "    \"<s/>\",\n",
        "    \"a\",\n",
        "    \"all\",\n",
        "    \"are\",\n",
        "    \"model\",\n",
        "    \"models\",\n",
        "    \"some\",\n",
        "    \"useful\",\n",
        "    \"wrong\"\n",
        "}\n",
        "\n",
        "def n_grams_with_good_turing():\n",
        "  unigram_counts = {}\n",
        "  total_unigrams = 0\n",
        "  for sentence in corpus:\n",
        "      tokens = preprocess_text(sentence)\n",
        "      for i in range(len(tokens)):\n",
        "          unigram = tokens[i]\n",
        "          total_unigrams += 1\n",
        "          unigram_counts[unigram] = unigram_counts.get(unigram, 0) + 1\n",
        "\n",
        "  counts_frequency = {}\n",
        "  for count in unigram_counts.values():\n",
        "    counts_frequency[count] = counts_frequency.get(count, 0) + 1\n",
        "\n",
        "  # Calculamos el N_r y r\n",
        "  N_r = {}\n",
        "  for r in counts_frequency:\n",
        "    N_r[r] = counts_frequency[r]\n",
        "\n",
        "  # Y calculamos r_star y N_r_star\n",
        "  r_star = {}\n",
        "  N_r_star = {}\n",
        "  for r in sorted(N_r.keys()):\n",
        "    if r + 1 in N_r:\n",
        "      r_star[r] = (r + 1) * N_r.get(r + 1, 0) / N_r.get(r, 0)\n",
        "      N_r_star[r] = N_r.get(r, 0)\n",
        "\n",
        "  print(\"Unigramas N_r y r_star: \")\n",
        "  for r, N_r_value in N_r.items():\n",
        "    print(f\"r: {r}, N_r: {N_r_value}, r_star: {r_star.get(r, 'N/A')}\")\n",
        "\n",
        "n_grams_with_good_turing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwPz78R7X1tM"
      },
      "source": [
        "# 0.2 Brown clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "iuSbQrBOYnSl",
        "outputId": "67caf7ce-391f-4fa4-a2f1-bfa756de8b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Reanudación', 'del', 'período', 'de', 'sesiones', 'Declaro', 'reanudado', 'el', 'período', 'de', 'sesiones', 'del', 'Parlamento', 'Europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'Sus', 'Señorías', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.'], ['Como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '\"', 'efecto', 'del', 'año', '2000', '\"', 'no', 'se', 'ha', 'producido', '.'], ['En', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'países', 'han', 'sido', 'víctimas', 'de', 'catástrofes', 'naturales', 'verdaderamente', 'terribles', '.'], ['Sus', 'Señorías', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'próximos', 'días', ',', 'en', 'el', 'curso', 'de', 'este', 'período', 'de', 'sesiones', '.'], ['A', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'víctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'países', 'de', 'la', 'Unión', 'Europea', 'afectados', '.'], ['Invito', 'a', 'todos', 'a', 'que', 'nos', 'pongamos', 'de', 'pie', 'para', 'guardar', 'un', 'minuto', 'de', 'silencio', '.'], ['(', 'El', 'Parlamento', ',', 'de', 'pie', ',', 'guarda', 'un', 'minuto', 'de', 'silencio', ')', 'Señora', 'Presidenta', ',', 'una', 'cuestión', 'de', 'procedimiento', '.'], ['Sabrá', 'usted', 'por', 'la', 'prensa', 'y', 'la', 'televisión', 'que', 'se', 'han', 'producido', 'una', 'serie', 'de', 'explosiones', 'y', 'asesinatos', 'en', 'Sri', 'Lanka', '.'], ['Una', 'de', 'las', 'personas', 'que', 'recientemente', 'han', 'asesinado', 'en', 'Sri', 'Lanka', 'ha', 'sido', 'al', 'Sr.', 'Kumar', 'Ponnambalam', ',', 'quien', 'hace', 'pocos', 'meses', 'visitó', 'el', 'Parlamento', 'Europeo', '.'], ['¿', 'Sería', 'apropiado', 'que', 'usted', ',', 'Señora', 'Presidenta', ',', 'escribiese', 'una', 'carta', 'al', 'Presidente', 'de', 'Sri', 'Lanka', 'expresando', 'las', 'condolencias', 'del', 'Parlamento', 'por', 'esa', 'y', 'otras', 'muertes', 'violentas', ',', 'pidiéndole', 'que', 'haga', 'todo', 'lo', 'posible', 'para', 'encontrar', 'una', 'reconciliación', 'pacífica', 'ante', 'la', 'extremadamente', 'difícil', 'situación', 'que', 'está', 'viviendo', 'su', 'país', '?'], ['Sí', ',', 'señor', 'Evans', ',', 'pienso', 'que', 'una', 'iniciativa', 'como', 'la', 'que', 'usted', 'acaba', 'de', 'sugerir', 'sería', 'muy', 'adecuada', '.'], ['Si', 'la', 'Asamblea', 'está', 'de', 'acuerdo', ',', 'haré', 'lo', 'que', 'el', 'señor', 'Evans', 'acaba', 'de', 'sugerir', '.', 'Señora', 'Presidenta', ',', 'una', 'cuestión', 'de', 'procedimiento', '.'], ['Me', 'gustaría', 'que', 'me', 'asesorara', 'sobre', 'el', 'Artículo', '143', 'concerniente', 'a', 'la', 'inadmisibilidad', '.'], ['Mi', 'pregunta', 'se', 'refiere', 'a', 'un', 'asunto', 'del', 'que', 'se', 'hablará', 'el', 'jueves', ',', 'día', 'que', 'en', 'volveré', 'a', 'plantearla', '.'], ['El', 'informe', 'Cunha', 'sobre', 'los', 'programas', 'de', 'dirección', 'plurianual', 'se', 'presenta', 'al', 'Parlamento', 'el', 'jueves', 'y', 'contiene', 'una', 'propuesta', 'en', 'el', 'apartado', '6', 'en', 'torno', 'a', 'una', 'forma', 'de', 'penalizaciones', 'basada', 'en', 'cuotas', 'que', 'debe', 'aplicarse', 'a', 'los', 'países', 'que', 'no', 'cumplan', 'anualmente', 'sus', 'objetivos', 'de', 'reducción', 'de', 'flota', '.'], ['El', 'informe', 'estipula', 'que', 'se', 'debe', 'aplicarse', 'a', 'pesar', 'del', 'principio', 'de', 'estabilidad', 'relativa', '.'], ['Creo', 'que', 'el', 'principio', 'de', 'estabilidad', 'relativa', 'es', 'un', 'principio', 'legal', 'fundamental', 'de', 'las', 'políticas', 'pesqueras', 'comunitarias', ',', 'por', 'lo', 'que', 'una', 'propuesta', 'que', 'lo', 'subvierta', 'es', 'legalmente', 'inadmisible', '.'], ['Quiero', 'saber', 'si', 'se', 'puede', 'hacer', 'este', 'tipo', 'de', 'objeción', 'a', 'lo', 'que', 'sólo', 'es', 'un', 'informe', ',', 'no', 'una', 'propuesta', 'legislativa', ',', 'y', 'si', 'es', 'algo', 'que', 'puedo', 'plantear', 'el', 'jueves', '.'], ['Su', 'Señoría', ',', 'si', 'así', 'lo', 'desea', ',', 'podrá', 'plantear', 'esta', 'cuestión', 'en', 'ese', 'momento', ',', 'es', 'decir', ',', 'el', 'jueves', 'antes', 'de', 'que', 'se', 'presente', 'el', 'informe', '.'], ['Señora', 'Presidenta', ',', 'coincidiendo', 'con', 'el', 'primer', 'período', 'parcial', 'de', 'sesiones', 'de', 'este', 'año', 'del', 'Parlamento', 'Europeo', ',', 'lamentablemente', ',', 'en', 'los', 'Estados', 'Unidos', ',', 'en', 'Texas', ',', 'se', 'ha', 'fijado', 'para', 'el', 'próximo', 'jueves', 'la', 'ejecución', 'de', 'un', 'condenado', 'a', 'la', 'pena', 'capital', ',', 'un', 'joven', 'de', '34', 'años', 'que', 'llamaremos', 'con', 'el', 'nombre', 'de', 'Hicks', '.']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-d7dc5f75b9ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "# El brown clusteing es un método de agrupamiento jerárquico\n",
        "# eque agrupo palabras basándose en eln contexto en el que\n",
        "# aparecen. Las palabras que aparecen en contextos similares tienden a tener significado similares\n",
        "# El objetivo es maximizar la probabilidada del corpus bajo un modelo\n",
        "# de lenguajge bigrama\n",
        "\n",
        "# El objetivo es encontar la asignación de clases que maximice\n",
        "# la verosimiiltud del corpus\n",
        "\n",
        "'''0.6.5. Ejercicio:'''\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "# Descargarmos algunos corpus\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download('cess_esp') # Este corpus está en español\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import cess_esp\n",
        "from nltk.corpus import europarl_raw\n",
        "from nltk.cluster import KMeansClusterer\n",
        "from nltk.cluster import euclidean_distance\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# En este caso utilizaremos el corpus europarl en español\n",
        "europarl_sentences = europarl_raw.spanish.sents()\n",
        "\n",
        "print(europarl_sentences[:20])\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    preprocessed = []\n",
        "    for sentence in sentences:\n",
        "        sentence = [word.lower() for word in sentence if word.isalpha()]\n",
        "        preprocessed.append(sentence)\n",
        "    return preprocessed\n",
        "\n",
        "preprocessed_sentences = preprocess_sentences(europarl_sentences)\n",
        "\n",
        "# Tokenización:\n",
        "def tokenize(text):\n",
        "  tokens = [[re.sub(r'[^\\w\\s]', '',t.lower()) for t in tex] for tex in text]\n",
        "  return tokens\n",
        "\n",
        "# Lematización\n",
        "def lemmatize(tokens):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [[lemmatizer.lemmatize(tok) for tok in token] for token in tokens]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "# Stemming\n",
        "def stemming(tokens):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [[stemmer.stem(tok) for tok in token] for token in tokens]\n",
        "  return stemmed_tokens\n",
        "\n",
        "# Remoción de stopwords\n",
        "stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "# Filter extrange words\n",
        "# En este caso se eliminarán las palabras que aparezcan menos de 5 veces\n",
        "# para reducir el tamaño del vocabulario\n",
        "def reduce_vocabulary_by_frequency(sentences, threshold=5):\n",
        "  word_counts = defaultdict(int)\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      word_counts[word] += 1\n",
        "\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    new_sentence = [word for word in sentence if word_counts[word] >= threshold]\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n",
        "\n",
        "# Generar n-gramas\n",
        "def generate_ngrams(sentence, n):\n",
        "    ngrams = zip(*[sentence[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "tokenized_sentences = remove_stop_words(tokenize(stemming(lemmatize(preprocessed_sentences[:500]))))\n",
        "\n",
        "print(tokenized_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "AcVggqSqkOFI",
        "outputId": "b8f06129-7cdf-4d8c-fa70-c2c5de19e0fd"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b829d43b6105>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m  \u001b[0;31m# Define el número de clusters deseado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mword_to_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meuroparl_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-b829d43b6105>\u001b[0m in \u001b[0;36mbrown_clustering\u001b[0;34m(sentences, num_clusters)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcluster1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcluster2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;31m# Calcular la reducción en la información mutua si se fusionan los clusters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0mmutual_information_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_mutual_information_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_to_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mmutual_information_reduction\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_mutual_information_reduction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-b829d43b6105>\u001b[0m in \u001b[0;36mcalculate_mutual_information_reduction\u001b[0;34m(sentences, cluster1, cluster2, cluster_to_words)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Calculamos la probabilidad de cada palabra en el cluster 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# y el cluster 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcurrent_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Update the offset table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m_read_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_sent_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_para_block_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36mread_blankline_block\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0;31m# End of file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0mstartpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0mnew_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Implementación de técnicas:\n",
        "\n",
        "# A. Brown clustering\n",
        "\n",
        "# Implementar el algoritmo de Brown Clustering para\n",
        "# agrupar palabras basándose en su contexto\n",
        "\n",
        "def brown_clustering(sentences, num_clusters):\n",
        "  # 1. Inicialización: Asignar cada palabra a su propio cluster.\n",
        "  word_to_cluster = {word: i for i, word in enumerate(set([word for sentence in sentences for word in sentence]))}\n",
        "  cluster_to_words = {i: [word] for i, word in enumerate(set([word for sentence in sentences for word in sentence]))}\n",
        "\n",
        "  # 2. Realizamos una iteración hasta que se alcance el número de clusters deseado.\n",
        "  while len(cluster_to_words) > num_clusters:\n",
        "    best_merge = None\n",
        "    max_mutual_information_reduction = float('-inf')\n",
        "\n",
        "    # 3. En casa paso o iteración se fusionan los dos clusters que resultan en la menor disminución del I(C)\n",
        "    for cluster1 in list(cluster_to_words.keys()):\n",
        "      for cluster2 in list(cluster_to_words.keys()):\n",
        "        if cluster1 != cluster2:\n",
        "          # Calcular la reducción en la información mutua si se fusionan los clusters.\n",
        "          mutual_information_reduction = calculate_mutual_information_reduction(sentences, cluster1, cluster2, cluster_to_words)\n",
        "\n",
        "          if mutual_information_reduction > max_mutual_information_reduction:\n",
        "            max_mutual_information_reduction = mutual_information_reduction\n",
        "            best_merge = (cluster1, cluster2)\n",
        "\n",
        "    # 4. Finalmente fusionamos los clusters con la mayor reducción en la información mutua.\n",
        "    if best_merge:\n",
        "      cluster1, cluster2 = best_merge\n",
        "      new_cluster = len(cluster_to_words)\n",
        "      cluster_to_words[new_cluster] = cluster_to_words[cluster1] + cluster_to_words[cluster2]\n",
        "      for word in cluster_to_words[cluster1] + cluster_to_words[cluster2]:\n",
        "        word_to_cluster[word] = new_cluster\n",
        "\n",
        "      del cluster_to_words[cluster1]\n",
        "      del cluster_to_words[cluster2]\n",
        "\n",
        "\n",
        "  return word_to_cluster\n",
        "\n",
        "def calculate_mutual_information_reduction(sentences, cluster1, cluster2, cluster_to_words):\n",
        "    words_in_cluster1 = cluster_to_words[cluster1]\n",
        "    words_in_cluster2 = cluster_to_words[cluster2]\n",
        "\n",
        "    # Inicializamos la reducción de la información mutua a 0\n",
        "    mutual_information_reduction = 0\n",
        "\n",
        "    # Calculamos la probabilidad de cada palabra en el cluster 1\n",
        "    # y el cluster 2\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            current_word = sentence[i]\n",
        "            next_word = sentence[i + 1]\n",
        "\n",
        "            if current_word in words_in_cluster1 and next_word in words_in_cluster2:\n",
        "                # Si la palabra actual esta en el cluster 1 y la\n",
        "                # siguiente palabra esta en el cluster 2\n",
        "\n",
        "                mutual_information_reduction += 1\n",
        "            elif current_word in words_in_cluster2 and next_word in words_in_cluster1:\n",
        "                # Si la palabra actual esta en el cluster 2 y la\n",
        "                # siguiente palabra esta en el cluster 1\n",
        "\n",
        "                mutual_information_reduction += 1\n",
        "\n",
        "    return mutual_information_reduction\n",
        "\n",
        "num_clusters = 3  # Define el número de clusters deseado\n",
        "\n",
        "word_to_cluster = brown_clustering(europarl_sentences, num_clusters)\n",
        "\n",
        "print(word_to_cluster)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFPQVGqskYhm"
      },
      "outputs": [],
      "source": [
        "# B. Latent semantic analysis (LSA)\n",
        "\n",
        "# Aplicar LSA para reducir la dimensionalidad y capturar\n",
        "# relaciones semánticas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LadjeWEio-NR"
      },
      "outputs": [],
      "source": [
        "# C. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7kgx9TCo97m"
      },
      "outputs": [],
      "source": [
        "# D. GloVe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1ZjIPIiEKvop",
        "bGmpf6fuLAaz",
        "OBdZLmmaLFuM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}